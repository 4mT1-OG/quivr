workflow_config:
  name: "standard RAG"
  nodes:
    - name: "START"
      conditional_edge:
        routing_function: "routing_split"
        conditions: ["edit_system_prompt", "filter_history"]

    - name: "edit_system_prompt"
      edges: ["filter_history"]

    - name: "filter_history"
      edges: ["rewrite"]

    - name: "rewrite"
      edges: ["retrieve"]

    - name: "retrieve"
      edges: ["generate_rag"]

    - name: "generate_rag" # the name of the last node, from which we want to stream the answer to the user, should always start with "generate"
      edges: ["END"]
# Maximum number of previous conversation iterations
# to include in the context of the answer
max_history: 10

# Number of chunks returned by the retriever
k: 40

# Whether to dynamically determine the number of chunks (from their relevance score)
# to be returned to the LLM for the answer generation
dynamic_chunk_retrieval: true

max_files: 20
reranker_config:
  # The reranker supplier to use
  supplier: "cohere"

  # The model to use for the reranker for the given supplier
  model: "rerank-multilingual-v3.0"

  # Only chunks with relevance scores equal or above this threshold will be
  # returned to the LLM to generate the answer (allowed values are between 0 and 1)
  relevance_score_threshold: 0.4

  # Number of chunks returned by the reranker
  top_n: 20
llm_config:

  # Maximum number of tokens to pass to the LLM
  # as a context to generate the answer
  max_context_tokens: 2000

  temperature: 0.7
  streaming: true

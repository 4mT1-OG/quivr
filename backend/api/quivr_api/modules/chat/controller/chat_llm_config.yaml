workflow_config:
  name: "Chat LLM"
  nodes:
    - name: "filter_history"
      edges: ["generate_chat_llm"]

    - name: "generate_chat_llm" # the name of the last node should always start with "generate"
      edges: ["END"]
# Maximum number of previous conversation iterations
# to include in the context of the answer
max_history: 10

#prompt: "my prompt"

llm_config:
  # The LLM supplier to use
  supplier: "openai"

  # The model to use for the LLM for the given supplier
  model: "gpt-3.5-turbo-0125"

  max_input_tokens: 2000

  # Maximum number of tokens to pass to the LLM
  # as a context to generate the answer
  max_output_tokens: 2000

  temperature: 0.7
  streaming: true
